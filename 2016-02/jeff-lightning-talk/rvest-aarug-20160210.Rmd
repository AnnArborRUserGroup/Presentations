---
title: "Scraping Web Content Using rvest"
author: "Jeff Shane"
date: "Feb 10, 2016"
output:
  ioslides_presentation:
    incremental: yes
    widescreen: yes
  slidy_presentation:
    incremental: yes
---

## About Me
- Data Scientist at Digital Roots
- M.A. Applied Statistics, 2012, UM

## Situation
- The Consumer Electronics Show (CES) is an annual conference that takes place in Las Vegas at the beginnning of January. My employer (Digital Roots) was a main social media provider for them.
- Many questions of interest:
  + What topics are people discussing?
  + What companies are getting buzz?
  + Which events are people at?
  + Which venues?
  + Customer assistance needs?
  + Public safety concerns?
  + ...

## Gathering industry types
- Which industries are getting interest?
  - Wearables, 3D printing, Audio, Autonomous Vehicles, ...
- A mention of a company can give us a clue about what industries are discussed
- To collect these categories for each of the ~3,800 companies at the show, we can scrape the CEs website


## About rvest
- Authored by Hadley Wickham (who else...)
- Wrappers around the 'xml2' and 'httr' packages to make it easy to download, then manipulate, HTML and XML

## Extracting text from an HTML node

```{r extractFromNode}

extractTextFromNode <- function(node, url)
{
  require(rvest)
  
  text <- 
    url %>% 
    read_html() %>% 
    html_nodes(node) %>% 
    html_text() 
  
  return(text)
}

```


## What is that argument in html_nodes()?
- A CSS selector, the node of how the webpage is structured
- We need to figure out which node has the information we want
- Use CSSSelectorGadget in Chrome is an easy way to find this


## Examples
```{r Examples}

exhId <- "T0011542"
baseUrl <- "http://ces16.mapyourshow.com/7_0/exhibitor/exhibitor-details.cfm?exhid="
url <- paste0(baseUrl, exhId)

# Company description
descript = extractTextFromNode(".mys-taper-measure", url)
cat(descript)
```

## Examples
```{r Examples2}
# Company categories
cats = extractTextFromNode(".mys-insideToggle", url)
cat(cats)

```


## End-to-end function
```{r end2end_scraper}
cesCompanyCategoryScraper <- function(exhId)
{
  require(rvest)

  baseUrl <- "http://ces16.mapyourshow.com/7_0/exhibitor/exhibitor-details.cfm?exhid="
  url <- paste0(baseUrl, exhId)
  
  categoriesRaw <- extractTextFromNode(".mys-insideToggle", url)

  categoriesClean <- categoriesRaw %>% 
    gsub(pattern = "\t", replacement = "") %>% 
    strsplit(split="\r\n") %>%
    unlist 
  
  if(length(categoriesClean) == 0)
  {
    return("")
  }
  
  categoriesClean = categoriesClean[categoriesClean != ""]
  
  return(categoriesClean)
  
}
```

## Extracting categories
```{r}
id = "T0011542"
cesCompanyCategoryScraper(id)
```


## Scraping tables
```{r tableFcn}

extractTableFromNode <- function(node, url)
{
  require(rvest)
  
  table = url %>% 
      read_html() %>% 
      html_nodes(node) %>% 
      html_table(header=TRUE)
    
  df = do.call(cbind.data.frame, table)
  
  return(df)
}

```


## Table example
```{r}
url = "https://cesweb.org/hotel"
node = "table"
tableRaw = extractTableFromNode(node, url)

head(tableRaw)
```

## Summary
- rvest makes the process of web scraping relatively simple
- Caution: sometimes the web text can't be gathered this way

