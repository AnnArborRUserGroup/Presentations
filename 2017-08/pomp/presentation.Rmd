---
title: "An Introduction to Hidden Markov Models with **pomp**"
author: "Ashton Baker"
date: "September 14, 2017"
output:
  slidy_presentation: default
  ioslides_presentation: default
  beamer_presentation: default
---

## Markov Chain
- A stochastic process that satisfies the Markov property
- All the information about the future state of the system is contained in the present state of the system.
- States of the system at sequential time points are related by a stochastic process with parameters $\theta$.

$$X_0 \rightarrow X_1 \rightarrow X_2 \rightarrow \cdots$$

## Markov Chain
- A stochastic process that satisfies tha Markov property
- All the information about the future state of the system is contained in the present state of the system.
- States of the system at sequential time points are related by a stochastic process with parameters $\theta$.

$$X_0 \overset{\theta}{\rightarrow} X_1 \overset{\theta}{\rightarrow} X_2 \overset{\theta}{\rightarrow} \cdots$$

## Markov Chain
- A stochastic process that satisfies tha Markov property
- All the information about the future state of the system is contained in the present state of the system.
- States of the system at sequential time points are related by a stochastic process with parameters $\theta$.

$$\begin{bmatrix}A_0\\B_0\\C_0\end{bmatrix} \overset{\theta}{\rightarrow} \begin{bmatrix}A_1\\B_1\\C_1\end{bmatrix} \overset{\theta}{\rightarrow} \begin{bmatrix}A_2\\B_2\\C_2\end{bmatrix} \overset{\theta}{\rightarrow} \cdots$$

## Markov Chain Example
```{r, out.width = "500px", echo=FALSE, fig.align='center'}
knitr::include_graphics("images/markov-chain.png")
```

## Markov Chain Example

Bull|Bear|Stagnant
----+----+--------
1   | 0  | 0
0   | 1  | 0
1   | 0  | 0
0   | 0  | 1
1   | 0  | 0
0   | 1  | 0
1   | 0  | 0
1   | 0  | 0

- Best way to estimate parameters in this case is to calculate MLE for the transition probabilities

## Hidden Markov Model
- The underlying system is a Markov process, with unobserved states.
- Data is generated by a (stochastic) observation process, and comprise "observed" states.
- Observation states and Process states may have any of a variety of relationships.

$$ \begin{matrix}
X_0        & \rightarrow & X_1        & \rightarrow & X_2        & \rightarrow & \cdots & \text{Signal}       \\
\downarrow &             & \downarrow &             & \downarrow &             &        &                     \\
Y_0        &             & Y_1        &             & Y_2        &             & \cdots & \text{Observations}
\end{matrix} $$

## Transcription Factor Binding Site Prediction

$$\begin{array}
\text{B} & B & B & B & 1 & 2 & 3 & 4 & B & \text{Signal} \\
A & C & G & A & A & C & T & C & A & \text{Observations}
\end{array}$$

```{r, out.width = "600px", fig.align='center', include=FALSE}
knitr::include_graphics("images/hmm-tfbs.png")
```
<footnote>Mathelier, A., & Wasserman, W. W. (2013). The next generation of transcription factor binding site prediction. PLoS computational biology, 9(9), e1003214.</footnote>

## Likelihood
- Calculated in the same way as probability
$$\mathcal{L}(\theta | x) = P(x | \theta)$$


## Likelihood for HMMs
$$ \begin{matrix}
X_0        & \rightarrow & X_1        & \rightarrow & X_2        & \rightarrow & \cdots        \\
           &             & \downarrow &             & \downarrow &             &               \\
           &             & Y_1        &             & Y_2        &             &
\end{matrix} $$

- $f_{X_0}$ : PDF of the system state at time $t=0$
- $f_{X}$ : PDF of $X_t$ given $X_{t-1}$
- $f_{Y}$ : PDF of $Y_t$ given $X_t$

- Probability of initial state $x_0$, state transition $x_0 \rightarrow x_1$, and observation $y_1$:
$$f_{X_0}(x_0; \theta) f_X(x_1 | x_0; \theta) f_Y(y_1 | x_1; \theta)$$

## Likelihood for HMMs
- Probability of initial state $x_0$, transitions $x_1, \ldots x_N$, and observations $y_1, \ldots, y_N$:
$$f_{X,Y}(x_{0:N}, y_{1:N}; \theta) = f_{X_0}(x_0; \theta) \prod_{t=1}^N f_X(x_t | x_{t-1}; \theta) f_Y(y_t | x_t; \theta)$$

- We have data $y_{1:N}^*$, but no observations for $x_{1:N}$. The solution is to compute the marginal PDF of $y_{1:N}$ and evaluate at the data $y_{1:N}^*$:

$$ \mathcal{L}(\theta) = \int_{x_{0:N}} f_{X,Y}(x_{0:N}, y_{1:N}^*; \theta) \; d x_{0:N}$$

## Likelihood for HMMs
We can sum the log-likelihood of successive transitions to obtain the log-likelihood of the entire sequence (due to the Markov property). But since the hidden state transitions are unknown, we have to sum over all possibilities. This is usually impossible.

$$ \begin{matrix}
X_0        & \rightarrow & X_1        & \rightarrow & X_2        & \rightarrow & \cdots        \\
           &             & \downarrow &             & \downarrow &             &               \\
           &             & 4          &             & 6       &             &
\end{matrix} $$

## Using **pomp** to build HMMs
Let's test `pomp` with some dummy data.
```{r}
p <- c('x0'=55, 'm'=2.56, 'sigma' = 8.23)
n <- 100

t = 1:n

x <- c(p['x0'])
for (i in 2:n) {
  x[i] <- x[i - 1] + p['m']
}

y <- c()
for (i in 1:n) {
  y[i] <- rnorm(1, x[i], sd = p['sigma'])
}

df <- data.frame(t, x, y)
```

## Using **pomp** to build HMMs
Hidden states (X)
```{r echo=F}
plot(df$x~df$t)
```

## Using **pomp** to build HMMs
Observed states (Y) with normally-distributed measurement error
```{r echo=F}
plot(df$y~df$t)
```